{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3e224c-c021-4591-ab7d-16c085fe4333",
   "metadata": {},
   "source": [
    "# Introduction to LangChain and MongoDB Atlas Vector Search\n",
    "URL: https://www.mongodb.com/developer/products/mongodb/langchain-vector-search/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19337a49-2359-4582-a36a-e1b9a4eed94f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2afd7be-a9a9-4d6d-95bb-bed7234b4aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules: `os` for environment variables, `load_dotenv` to load .env files,\n",
    "# and `MongoClient` for MongoDB operations.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load environment variables from a .env file, overriding any existing variables in the environment.\n",
    "# This is useful for not cluttering the code with sensitive information like API keys and database URIs.\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Note for users: You should create a .env file in the notebook's root directory\n",
    "# with the content MONGO_URI=\"your_mongodb_uri\" to load the MONGO_URI variable.\n",
    "\n",
    "# Retrieve the OPENAI_API_KEY and MONGO_URI from the environment variables.\n",
    "# These are critical for accessing OpenAI's API and the MongoDB database, respectively.\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "MONGO_URI = os.environ[\"MONGO_URI\"]\n",
    "\n",
    "# Define constants for database and collection names, and the name of the vector search index.\n",
    "# These will be used to specify where the data should be stored and how to manage search indexes in MongoDB.\n",
    "DB_NAME = \"langchain-test-2\"\n",
    "COLLECTION_NAME = \"test\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"default\"\n",
    "\n",
    "# Define the name of the field where embeddings will be stored in the database documents.\n",
    "# This is important for vector searches that rely on these embeddings.\n",
    "EMBEDDING_FIELD_NAME = \"embedding\"\n",
    "\n",
    "# Establish a connection to MongoDB using the URI from the environment variables.\n",
    "# This client object will be used for all database operations.\n",
    "client = MongoClient(MONGO_URI)\n",
    "\n",
    "# Access the specific database and collection within MongoDB where operations will be performed.\n",
    "db = client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "\n",
    "# Attempt to ping the MongoDB deployment to confirm a successful connection.\n",
    "# This is a good practice to verify that the client is properly configured and can communicate with the database.\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)  # If the connection fails, print the error message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662431c-96ca-489f-94e1-38663f63fee4",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fd82e-edf5-4708-a571-d0adf9f49330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for document loading, text splitting, and embedding generation.\n",
    "from langchain.document_loaders import PyPDFLoader  # For loading PDF documents.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting text into manageable chunks.\n",
    "from langchain_openai import OpenAIEmbeddings  # For generating text embeddings using OpenAI's models.\n",
    "from langchain.vectorstores import MongoDBAtlasVectorSearch  # For storing and searching document vectors.\n",
    "\n",
    "# Load a PDF document from a URL, specifically an arXiv paper in this case.\n",
    "# The PyPDFLoader class is used to fetch and read the content of the PDF document from the specified URL.\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2303.08774.pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split the loaded document text into smaller chunks for processing.\n",
    "# A RecursiveCharacterTextSplitter is used to divide the text into chunks of 500 characters,\n",
    "# with a 50 character overlap between consecutive chunks to ensure continuity.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "# Use MongoDBAtlasVectorSearch to insert the document chunks into a MongoDB collection.\n",
    "# Each chunk is treated as a separate document.\n",
    "# The `from_documents` method is used here, which takes the documents (chunks),\n",
    "# generates embeddings for them using OpenAIEmbeddings, and inserts them into the specified collection.\n",
    "# This operation also involves creating or updating the specified vector search index.\n",
    "x = MongoDBAtlasVectorSearch.from_documents(\n",
    "    documents=docs,  # The chunks of text to be inserted.\n",
    "    embedding=OpenAIEmbeddings(disallowed_special=()),  # The embedding model to use.\n",
    "    collection=collection,  # The MongoDB collection where the documents are to be inserted.\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME  # The name of the vector search index.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2f12e-603f-4d5c-93e8-bf43a91087e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the data was loade\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f8adf-01c9-4f4e-83c2-6e944eab0432",
   "metadata": {},
   "source": [
    "## Create the Vector search index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3770b87-7496-4c51-89ee-d499280f0d79",
   "metadata": {},
   "source": [
    "Let’s head over to our MongoDB Atlas user interface to create our Vector Search Index. First, click on the “Search” tab and then on “Create Search Index.” You’ll be taken to this page. Please click on “JSON Editor.”\n",
    "\n",
    "Check out our \n",
    "Vector Search documentation\n",
    " for more information on the index configuration settings\n",
    " https://www.mongodb.com/docs/atlas/atlas-search/vector-search/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fa750-0e48-4465-8dce-ef69911d9752",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"type\": \"vector\",\n",
    "      \"path\": \"embedding\",  \n",
    "      \"numDimensions\": 1536, \n",
    "      \"similarity\": \"cosine\"  \n",
    "    },\n",
    "    {\n",
    "      \"type\": \"filter\",\n",
    "      \"path\": \"page\"  \n",
    "    }\n",
    "   \n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bac488-994b-42f2-9727-c91c457bb283",
   "metadata": {},
   "source": [
    "These fields are to specify the field name in our documents. With *embedding*, we are specifying that the dimensions of the model used to embed are *1536*, and the similarity function used to find the nearest k neighbors is *cosine*. It’s crucial that the dimensions in our search index match that of the language model we are using to embed our data.\n",
    "\n",
    "We are also adding metadata fields for filtering, this will allow us to use pre-filtering in our RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d2f4c-bcdf-45aa-ac28-afe38a5f31c2",
   "metadata": {},
   "source": [
    "## Querying our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991921a2-4ade-43b1-b778-755350d5781a",
   "metadata": {},
   "source": [
    "### Semantic search in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aea8e72-9114-4608-aa58-cb5c2bf4bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for working with OpenAI embeddings and MongoDB Atlas vector search.\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "\n",
    "# Initialize a MongoDB Atlas vector search object.\n",
    "# This involves connecting to a MongoDB Atlas instance using a connection string and specifying the database and collection names.\n",
    "# The `OpenAIEmbeddings` object is used to generate vector embeddings for documents, with no special tokens disallowed.\n",
    "# An index name for the Atlas vector search is also provided.\n",
    "vector_search = MongoDBAtlasVectorSearch.from_connection_string(\n",
    "   MONGO_URI,  # Placeholder for the actual MongoDB URI connection string.\n",
    "   DB_NAME + \".\" + COLLECTION_NAME,  # The target database and collection in \"db.collection\" format.\n",
    "   OpenAIEmbeddings(disallowed_special=()),  # Initialize OpenAI embeddings without disallowing any special tokens.\n",
    "   index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME  # The name of the index to use for vector searches.\n",
    ")\n",
    "\n",
    "# Define the query to be used for the similarity search. In this case, looking for documents similar to \"gpt-4\".\n",
    "query = \"gpt-4\"\n",
    "\n",
    "# Execute the similarity search against the specified collection using the given query.\n",
    "# The search is limited to the top 20 most similar results (`k=20`).\n",
    "results = vector_search.similarity_search(\n",
    "   query=query,\n",
    "   k=20,\n",
    ")\n",
    "\n",
    "# Iterate through the results of the similarity search.\n",
    "# For each result, print the page content followed by a newline for separation.\n",
    "# This loop effectively outputs the content of each document that was found to be similar to the query.\n",
    "for result in results:\n",
    "   print(result.page_content + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7503d9e0-f1bb-4eef-bef3-5fb3da23e496",
   "metadata": {},
   "source": [
    "This gives us the relevant results that semantically match the intent behind the question. Now, let’s see what happens when we ask a question using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b29272-a19d-4ce2-8bf7-0c13af1c3d25",
   "metadata": {},
   "source": [
    "### Question and answering in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2e638-ee0b-4900-8b7e-e66a81b57b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a document retriever for semantic similarity searches.\n",
    "# It's configured to retrieve the top 200 similar documents based on the query (`\"k\": 200`).\n",
    "# After the initial retrieval, a post-filtering pipeline is applied that first limits the results to the top 2 (`\"$limit\": 2`),\n",
    "# and then removes the 'embedding' data from those results for cleaner output (`\"$project\": {\"embedding\": 0}`).\n",
    "qa_retriever = vector_search.as_retriever(\n",
    "   search_type=\"similarity\",\n",
    "   search_kwargs={\n",
    "       \"k\": 200,\n",
    "       \"post_filter_pipeline\": [{\"$limit\": 2},{\"$project\" : {\"embedding\" : 0}}]\n",
    "   }\n",
    ")\n",
    "\n",
    "# Import the PromptTemplate class, which is used to create structured prompts for the language model.\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a prompt template that structures how the context and the question will be presented to the language model.\n",
    "# This ensures the language model understands it's to use the provided context to answer the given question.\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object with the defined template, specifying the variables that will be dynamically filled in.\n",
    "PROMPT = PromptTemplate(\n",
    "   template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Import the necessary classes for creating a retrieval-based QA system.\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Configure the QA system. This involves setting the language model (llm) to use (here, ChatOpenAI backed by OpenAI),\n",
    "# specifying the type of chain (arbitrarily named \"stuff\"), the document retriever,\n",
    "# indicating that source documents should be returned with the answer,\n",
    "# and providing the prompt template to be used with the language model.\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(),chain_type=\"stuff\", retriever=qa_retriever, return_source_documents=True, chain_type_kwargs={\"prompt\": PROMPT})\n",
    "\n",
    "# Use the QA system to answer a question about GPT-4.\n",
    "# The system retrieves relevant documents based on semantic similarity and then formulates an answer based on those documents.\n",
    "docs = qa({\"query\": \"What is gpt4?\"})\n",
    "\n",
    "# Print the answer generated by the QA system.\n",
    "print(docs[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe836f-d62b-4ba1-93bd-607a733a670d",
   "metadata": {},
   "source": [
    "### Pre-filtering with Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beadeadc-01f3-4ec6-8d8d-c8a48bd4392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a retriever using vector search for document similarity.\n",
    "# This retriever will find documents based on their semantic similarity to a query.\n",
    "# It is configured to retrieve up to 200 documents (`\"k\": 200`) that match the specified pre-filter criteria.\n",
    "# The pre-filter here ensures that only documents from page 3 (`\"page\": {\"$eq\": 3}`) are considered.\n",
    "# After retrieval, a post-filter pipeline is applied to keep only the top 2 results (`\"$limit\": 2`) and\n",
    "# remove the `embedding` field from those results (`\"$project\": {\"embedding\": 0}`).\n",
    "qa_retriever = vector_search.as_retriever(\n",
    "   search_type=\"similarity\",\n",
    "   search_kwargs={\n",
    "       \"k\": 200,\n",
    "       \"pre_filter\": {\"page\": { \"$eq\" : 3}},\n",
    "       \"post_filter_pipeline\": [{\"$limit\": 2},{\"$project\" : {\"embedding\" : 0}}]\n",
    "   }\n",
    ")\n",
    "\n",
    "# Import the PromptTemplate class, which is used to format the input for the language model.\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a prompt template for the QA system.\n",
    "# This template formats the context and question into a structured format that the language model can understand.\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object using the defined template and specifying the input variables.\n",
    "PROMPT = PromptTemplate(\n",
    "   template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Import necessary modules for setting up the retrieval-based QA system.\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Set up the QA system by specifying the language model (ChatOpenAI powered by OpenAI),\n",
    "# the type of chain (arbitrarily named \"stuff\" here), the document retriever, and whether to return the source documents.\n",
    "# The `chain_type_kwargs` argument is used to pass the prompt template to the system.\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(),chain_type=\"stuff\", retriever=qa_retriever, return_source_documents=True, chain_type_kwargs={\"prompt\": PROMPT})\n",
    "\n",
    "# Execute the QA system with a query asking about GPT-4.\n",
    "# The query is processed by the retrieval system to find relevant documents, which are then used by the language model to generate an answer.\n",
    "docs = qa({\"query\": \"What is gpt4?\"})\n",
    "\n",
    "# Print the result of the QA system, which should include the answer to the query along with any source documents if configured to return them.\n",
    "print(docs[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99d123-361a-47bd-9ecd-97f34f14d4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
